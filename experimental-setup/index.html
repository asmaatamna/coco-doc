

<!doctype html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>COCO: The Experimental Procedure &mdash; COCO: The Experimental Procedure</title>
    
    <link rel="stylesheet" href="_static/bizstyle.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.9',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/bizstyle.js"></script>
    <link rel="top" title="COCO: The Experimental Procedure" href="#" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <!--[if lt IE 9]>
    <script type="text/javascript" src="_static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: The Experimental Procedure</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">COCO: The Experimental Procedure</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a><ul>
<li><a class="reference internal" href="#terminology">Terminology</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conducting-the-experiment">Conducting the Experiment</a><ul>
<li><a class="reference internal" href="#initialization-and-input-to-the-algorithm">Initialization and Input to the Algorithm</a></li>
<li><a class="reference internal" href="#budget-termination-criteria-and-restarts">Budget, Termination Criteria, and Restarts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#parameter-setting-and-tuning-of-algorithms">Parameter Setting and Tuning of Algorithms</a></li>
<li><a class="reference internal" href="#recommendations">Recommendations</a></li>
<li><a class="reference internal" href="#time-complexity-experiment">Time Complexity Experiment</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="coco-the-experimental-procedure">
<h1>COCO: The Experimental Procedure<a class="headerlink" href="#coco-the-experimental-procedure" title="Permalink to this headline">¶</a></h1>
See also: <I>ArXiv e-prints</I>,
<A HREF="http://arxiv.org/abs/1603.08776">arXiv:1603.08776</A>, 2016.<p>We present a budget-free experimental setup and procedure for benchmarking numerical
optimization algorithms in a black-box scenario.
This procedure can be applied with the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> benchmarking platform.
We describe initialization of and input to the algorithm and touch upon the
relevance of termination and restarts.
We finally reconsider parameter tuning and the concept of recommendations for
benchmarking with <a class="reference external" href="https://github.com/numbbo/coco">COCO</a>.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Based on <a class="reference internal" href="#han2009" id="id4">[HAN2009]</a> and <a class="reference internal" href="#han2010" id="id5">[HAN2010]</a>, we describe a comparatively simple experimental
set-up for <em>black-box optimization benchmarking</em>. We recommend to use this procedure
within the <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> platform <a class="reference internal" href="#han2016co" id="id6">[HAN2016co]</a>. <a class="footnote-reference" href="#id10" id="id7">[1]</a></p>
<p>Our <strong>central measure of performance</strong>, to which the experimental procedure is
adapted, is the number of calls to the objective function to reach a
certain solution quality (function value or <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value or indicator
value), also denoted as runtime.</p>
<div class="section" id="terminology">
<h3>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h3>
<dl class="docutils">
<dt><em>function</em></dt>
<dd>We talk about an objective <em>function</em> <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/> as a parametrized mapping
<img class="math" src="_images/math/bb3b4cc088220f561d9624697122d49cf057f0b2.png" alt="\mathbb{R}^n\to\mathbb{R}^m" style="vertical-align: -1px"/> with scalable input space, that is,
<img class="math" src="_images/math/273f486f7ce4023c108fff2c278487a34aaa6321.png" alt="n" style="vertical-align: 0px"/> is not (yet) determined, and usually <img class="math" src="_images/math/64a1faffe7792d85f77af7923e09a5c0b82bb1fc.png" alt="m\in\{1,2\}" style="vertical-align: -5px"/>.
Functions are parametrized such that different <em>instances</em> of the
&#8220;same&#8221; function are available, e.g. translated or shifted versions.</dd>
<dt><em>problem</em></dt>
<dd>We talk about a <em>problem</em>, <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a408ba01b98c78bf5be3df36562d99478"><code class="docutils literal"><span class="pre">coco_problem_t</span></code></a>, as a specific <em>function
instance</em> on which the optimization algorithm is run. Specifically, a problem
can be described as the triple <code class="docutils literal"><span class="pre">(dimension,</span> <span class="pre">function,</span> <span class="pre">instance)</span></code>. A problem
can be evaluated and returns an <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value or -vector.
In the context of performance
assessment, a target <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>- or indicator-value
is attached to each problem. That is, a target value is added to the
above triple to define a single problem in this case.</dd>
<dt><em>runtime</em></dt>
<dd>We define <em>runtime</em>, or <em>run-length</em> <a class="reference internal" href="#hoo1998" id="id8">[HOO1998]</a>
as the <em>number of evaluations</em>
conducted on a given problem, also referred to as number of <em>function</em> evaluations.
Our central performance measure is the runtime until a given target value
is hit <a class="reference internal" href="#han2016perf" id="id9">[HAN2016perf]</a>.</dd>
<dt><em>suite</em></dt>
<dd>A test- or benchmark-suite is a collection of problems, typically between
twenty and a hundred, where the number of objectives <img class="math" src="_images/math/862c9e331021da02c140ce6caef6f1d6519f571c.png" alt="m" style="vertical-align: 0px"/> is fixed.</dd>
</dl>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[1]</a></td><td>The <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> platform provides
several (single and bi-objective) <em>test suites</em> with a collection of
black-box optimization problems of different dimensions to be
minimized. <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> automatically collects the relevant data to display
the performance results after a post-processing is applied.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="conducting-the-experiment">
<h2>Conducting the Experiment<a class="headerlink" href="#conducting-the-experiment" title="Permalink to this headline">¶</a></h2>
<p>The optimization algorithm to be benchmarked is run on each problem of the
given test suite once. On each problem, the very same algorithm with the same
parameter setting, the same initialzation procedure, the same budget, the same
termination and/or restart criteria etc. is used.
There is no prescribed minimal or maximal allowed budget, the benchmarking approach is <em>budget-free</em>.
The longer the experiment, the more data are available to assess the performance
accurately.
See also Section <a class="reference internal" href="#sec-budget"><span>Budget, Termination Criteria, and Restarts</span></a>.</p>
<div class="section" id="initialization-and-input-to-the-algorithm">
<span id="sec-input"></span><h3>Initialization and Input to the Algorithm<a class="headerlink" href="#initialization-and-input-to-the-algorithm" title="Permalink to this headline">¶</a></h3>
<p>An algorithm can use the following input information from each problem.
At any time:</p>
<dl class="docutils">
<dt><em>Input and output dimensions</em></dt>
<dd><p class="first">as a defining interface to the problem, specifically:</p>
<blockquote class="last">
<div><ul class="simple">
<li>The search space (input) dimension via <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a0dabf3e4f5630d08077530a1341f13ab"><code class="docutils literal"><span class="pre">coco_problem_get_dimension</span></code></a>,</li>
<li>The number of objectives via <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#ab0d1fcc7f592c283f1e67cde2afeb60a"><code class="docutils literal"><span class="pre">coco_problem_get_number_of_objectives</span></code></a>,
which is the &#8220;output&#8221; dimension of <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#aabbc02b57084ab069c37e1c27426b95c"><code class="docutils literal"><span class="pre">coco_evaluate_function</span></code></a>.
All functions of a single benchmark suite have the same number
of objectives, currently either one or two.</li>
<li>The number of constraints via <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#ad5c7b0889170a105671a14c8383fbb22"><code class="docutils literal"><span class="pre">coco_problem_get_number_of_constraints</span></code></a>,
which is the &#8220;output&#8221; dimension of <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#ab5cce904e394349ec1be1bcdc35967fa"><code class="docutils literal"><span class="pre">coco_evaluate_constraint</span></code></a>. <em>All</em>
problems of a single benchmark suite have either no constraints, or
one or more constraints.</li>
</ul>
</div></blockquote>
</dd>
<dt><em>Search domain of interest</em></dt>
<dd>defined from <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a29c89e039494ae8b4f8e520cba1eb154"><code class="docutils literal"><span class="pre">coco_problem_get_largest_values_of_interest</span></code></a> and <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a4ea6c067adfa866b0179329fe9b7c458"><code class="docutils literal"><span class="pre">coco_problem_get_smallest_values_of_interest</span></code></a>. The optimum (or each extremal solution of the Pareto set) lies within the search domain of interest. If the optimizer operates on a bounded domain only, the domain of interest can be interpreted as lower and upper bounds.</dd>
<dt><em>Feasible (initial) solution</em></dt>
<dd>provided by <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#ac5a44845acfadd7c5cccb9900a566b32"><code class="docutils literal"><span class="pre">coco_problem_get_initial_solution</span></code></a>.</dd>
</dl>
<p>The initial state of the optimization algorithm and its parameters shall only be based on
these input values. The initial algorithm setting is considered as part of
the algorithm and must therefore follow the same procedure for all problems of the
suite. The problem identifier or the positioning of the problem in the suite or
any (other) known characteristics of the problem are not
allowed as input to the algorithm, see also Section
<a class="reference internal" href="#sec-tuning"><span>Parameter Setting and Tuning of Algorithms</span></a>.</p>
<p>During an optimization run, the following (new) information is available to
the algorithm:</p>
<ol class="arabic simple">
<li>The result, i.e. the <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value(s), from evaluating the problem
at a given search point
via <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#aabbc02b57084ab069c37e1c27426b95c"><code class="docutils literal"><span class="pre">coco_evaluate_function</span></code></a>.</li>
<li>The result from evaluating the constraints of the problem at a
given search point via <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#ab5cce904e394349ec1be1bcdc35967fa"><code class="docutils literal"><span class="pre">coco_evaluate_constraint</span></code></a>.</li>
<li>The result of <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a1164d85fd641ca48046b943344ae9069"><code class="docutils literal"><span class="pre">coco_problem_final_target_hit</span></code></a>, which can be used
to terminate a run conclusively without changing the performance assessment
in any way. Currently, if the number of objectives <img class="math" src="_images/math/e6a1ed62060cf69c85058c322893480f524a68d5.png" alt="m &gt; 1" style="vertical-align: -1px"/>, this
function returns always zero.</li>
</ol>
<p>The number of evaluations of the problem and/or constraints are the search
costs, also referred to as <em>runtime</em>, and used for the performance
assessment of the algorithm. <a class="footnote-reference" href="#id12" id="id11">[2]</a></p>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id11">[2]</a></td><td><a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a6ad88cdba2ffd15847346d594974067f"><code class="docutils literal"><span class="pre">coco_problem_get_evaluations(const</span> <span class="pre">coco_problem_t</span> <span class="pre">*</span> <span class="pre">problem)</span></code></a> is a
convenience function that returns the number of evaluations done on <code class="docutils literal"><span class="pre">problem</span></code>.
Because this information is available to the optimization algorithm anyway,
the convenience function might be used additionally.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="budget-termination-criteria-and-restarts">
<span id="sec-budget"></span><h3>Budget, Termination Criteria, and Restarts<a class="headerlink" href="#budget-termination-criteria-and-restarts" title="Permalink to this headline">¶</a></h3>
<p>We consider the budget, termination criteria, and restarts to be part of the
benchmarked algorithm. Algorithms with any budget of function evaluations are
eligible, our benchmarking setup is budget-free.
The choice of termination is a relevant part of the algorithm.
On the one hand, allowing a larger number of function evaluations increases the chance to achieve better function values. On the other hand, a timely
termination of a stagnating run can improve the performance, as these evaluations
can be used more effectively. <a class="footnote-reference" href="#id19" id="id13">[3]</a></p>
<p>To exploit a large number of function evaluations effectively, we encourage to
use <strong>independent restarts</strong> <a class="footnote-reference" href="#id20" id="id14">[4]</a>, in particular for algorithms which terminate
naturally within a comparatively small budget.
Independent restarts are a natural way to approach difficult optimization
problems and do not change the central performance measure used in <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> (hence
we call <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> <em>budget-free</em>),
however, they improve the reliability, comparability <a class="footnote-reference" href="#id21" id="id15">[5]</a>, precision, and &#8220;visibility&#8221; of the measured results.</p>
<p>Moreover, any <strong>multistart procedure</strong> (which relies on an interim termination of the algorithm) is encouraged.
Multistarts may not be independent as they can feature a parameter sweep (e.g., increasing population size <a class="reference internal" href="#har1999" id="id16">[HAR1999]</a> <a class="reference internal" href="#aug2005" id="id17">[AUG2005]</a>), can be based on the outcome of the previous starts, or feature a systematic change of the initial conditions for the algorithm.</p>
<p>After a multistart procedure has been established, a recommended procedure is
to use a budget proportional to the dimension, <img class="math" src="_images/math/5a32ef7e8ff1eeb54f16f0ac56e2db91796bd804.png" alt="k\times n" style="vertical-align: 0px"/>, and run
repeated experiments with increase <img class="math" src="_images/math/93ad71e1e72170c026513e0449297ebab47bec72.png" alt="k" style="vertical-align: 0px"/>, e.g. like
<img class="math" src="_images/math/c59972ea8d838c75d28762ad384d22971a076310.png" alt="3, 10, 30, 100, 300,\dots" style="vertical-align: -4px"/>, which is a good compromise between
availability of the latest results and computational overhead.</p>
<p>An algorithm can be conclusively terminated if
<a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#a1164d85fd641ca48046b943344ae9069"><code class="docutils literal"><span class="pre">coco_problem_final_target_hit</span></code></a> returns 1. <a class="footnote-reference" href="#id22" id="id18">[6]</a> This saves CPU cycles without
affecting the performance assessment, because there is no target left to hit.</p>
<table class="docutils footnote" frame="void" id="id19" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id13">[3]</a></td><td>In the single objective case care should be
taken to apply termination conditions that allow to hit the final target on
the most basic functions, like the sphere function <img class="math" src="_images/math/32f0d9da4470914afa0d5ee26b95e81f059e8587.png" alt="f_1" style="vertical-align: -4px"/>, that is on the
problems 0, 360, 720, 1080, 1440, and 1800 of the <code class="docutils literal"><span class="pre">bbob</span></code> suite.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id14">[4]</a></td><td>The <a class="reference external" href="https://github.com/numbbo/coco">COCO</a> platform provides example code implementing independent restarts.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[5]</a></td><td>Algorithms are only comparable up to the smallest budget given to
any of them.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id18">[6]</a></td><td>For the <code class="docutils literal"><span class="pre">bbob-biobj</span></code> suite this is however currently never the case.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="parameter-setting-and-tuning-of-algorithms">
<span id="sec-tuning"></span><h2>Parameter Setting and Tuning of Algorithms<a class="headerlink" href="#parameter-setting-and-tuning-of-algorithms" title="Permalink to this headline">¶</a></h2>
<p>Any tuning of algorithm parameters to the test suite should be described and
<em>the approximate overall number of tested parameter settings or algorithm
variants and the approximate overall invested budget should be given</em>.</p>
<p>The only recommended tuning procedure is the verification that <strong>termination
conditions</strong> of the algorithm are suited to the given testbed and, in case,
tuning of termination parameters. <a class="footnote-reference" href="#id24" id="id23">[7]</a>
Too early or too late termination can be identified and adjusted comparatively
easy.
This is also a useful prerequisite for restarts, see also above.</p>
<p>On all functions the very same parameter setting must be used (which might
well depend on the dimensionality, see Section <a class="reference internal" href="#sec-input"><span>Initialization and Input to the Algorithm</span></a>). That means,
the <em>a priori</em> use of function-dependent parameter settings is prohibited
(since 2012).  The function ID or any function characteristics (like
separability, multi-modality, ...) cannot be considered as input parameter to
the algorithm.</p>
<p>On the other hand, benchmarking different parameter settings as &#8220;different
algorithms&#8221; on the entire test suite is encouraged.</p>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[7]</a></td><td>In our experience, numerical optimization software frequently terminates
too early by default, while evolutionary computation software often
terminates too late by default.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="recommendations">
<span id="sec-recommendations"></span><h2>Recommendations<a class="headerlink" href="#recommendations" title="Permalink to this headline">¶</a></h2>
<p>The performance assessment is based on a set of evaluation counts
associated with the <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value or -vector of a solution.
By default, each evaluation count is associated with the respectively <em>evaluated</em>
solution and hence its <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value.
In the single-objective case, the solution associated <em>to the current (last)
evaluation</em> can be changed by calling <a class="reference external" href="http://numbbo.github.io/coco-doc/C/coco_8h.html#afd76a19eddd49fb78c22563390437df2"><code class="docutils literal"><span class="pre">coco_recommend_solution</span></code></a>, thereby
associating the <img class="math" src="_images/math/f0307af130a9303d3def5b2c46714d19118bac36.png" alt="f" style="vertical-align: -4px"/>-value of the <em>recommended</em> solution (instead of the
<em>evaluated</em> solution) with the current evaluation count.
A recommendation is best viewed as the <em>currently best known approximation</em> of the
optimum <a class="footnote-reference" href="#id26" id="id25">[8]</a> delivered by the optimization algorithm, or as the currently most
desirable return value of the algorithm.</p>
<p>Recommendations allow the algorithm to explore solutions without affecting the
performance assessment. For example, a surrogate-based algorithm can explore
(i.e. evaluate) an arbitrarily bad solution, update the surrogate model and
then recommend the (new) model optimizer. On non-noisy suites it is neither
necessary nor advantageous to recommend the same solution repeatedly.</p>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id25">[8]</a></td><td>In the noisy scenario, a small number of the most current solutions
will be taken into account <a class="reference internal" href="#han2016perf" id="id27">[HAN2016perf]</a>.
In the multi-objective scenario, the recommendation option is not available,
because an archive of non-dominated solutions presumes that all solutions are
evaluated.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="time-complexity-experiment">
<h2>Time Complexity Experiment<a class="headerlink" href="#time-complexity-experiment" title="Permalink to this headline">¶</a></h2>
<p>In order to get a rough measurement of the time complexity of the algorithm, the
wall-clock or CPU time should be measured when running the algorithm on the
benchmark suite. The chosen setup should reflect a &#8220;realistic average
scenario&#8221;. <a class="footnote-reference" href="#id29" id="id28">[9]</a>
The <em>time divided by the number of function evaluations shall be presented
separately for each dimension</em>. The chosen setup, coding language, compiler and
computational architecture for conducting these experiments should be given.</p>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id28">[9]</a></td><td>The example experiment code provides the timing output measured over all
problems of a single dimension by default. It also can be used to make a record
of the same timing experiment with &#8220;pure random search&#8221;, which can serve as
additional base-line data. On the <code class="docutils literal"><span class="pre">bbob</span></code> test suite, also only the
first instance of the Rosenbrock function <img class="math" src="_images/math/7287e68c96125b4964bddd3a5591f09be5dfd24d.png" alt="f_8" style="vertical-align: -4px"/> had been used for this
experiment previously, that is, the suite indices 105, 465, 825, 1185, 1545,
1905.</td></tr>
</tbody>
</table>
<H2>Acknowledgments</H2><p>The authors would like to thank Raymond Ros, Steffen Finck, Marc Schoenauer,
Petr Posik and Dejan Tušar for their many invaluable contributions to this work.</p>
<p>This work was support by the grant ANR-12-MONU-0009 (NumBBO)
of the French National Research Agency.</p>
<H2>References</H2><table class="docutils citation" frame="void" id="aug2005" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[AUG2005]</a></td><td>A. Auger and N. Hansen. A restart CMA evolution strategy with
increasing population size. In <em>Proceedings of the IEEE Congress on
Evolutionary Computation (CEC 2005)</em>, pages 1769&#8211;1776. IEEE Press, 2005.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2016perf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HAN2016perf]</td><td><em>(<a class="fn-backref" href="#id9">1</a>, <a class="fn-backref" href="#id27">2</a>)</em> N. Hansen, A. Auger, D. Brockhoff, D. Tušar, T. Tušar.
<a class="reference external" href="http://numbbo.github.io/coco-doc/perf-assessment">COCO: Performance Assessment</a>. <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1605.03560">arXiv:1605.03560</a>, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[HAN2009]</a></td><td>N. Hansen, A. Auger, S. Finck, and R. Ros.
Real-Parameter Black-Box Optimization Benchmarking 2009: Experimental Setup, <em>Inria Research Report</em> RR-6828 <a class="reference external" href="http://hal.inria.fr/inria-00362649/en">http://hal.inria.fr/inria-00362649/en</a>, 2009.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2010" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[HAN2010]</a></td><td>N. Hansen, A. Auger, S. Finck, and R. Ros.
Real-Parameter Black-Box Optimization Benchmarking 2010: Experimental Setup, <em>Inria Research Report</em> RR-7215 <a class="reference external" href="http://hal.inria.fr/inria-00362649/en">http://hal.inria.fr/inria-00362649/en</a>, 2010.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="han2016co" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[HAN2016co]</a></td><td>N. Hansen, A. Auger, O. Mersmann, T. Tušar, D. Brockhoff.
<a class="reference external" href="http://numbbo.github.io/coco-doc/">COCO: A Platform for Comparing Continuous Optimizers in a Black-Box
Setting</a>, <em>ArXiv e-prints</em>, <a class="reference external" href="http://arxiv.org/abs/1603.08785">arXiv:1603.08785</a>, 2016.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="har1999" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[HAR1999]</a></td><td>G.R. Harik and F.G. Lobo. A parameter-less genetic
algorithm. In <em>Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO)</em>, volume 1, pages 258-265. ACM, 1999.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hoo1998" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[HOO1998]</a></td><td>H.H. Hoos and T. Stützle. Evaluating Las Vegas
algorithms: pitfalls and remedies. In <em>Proceedings of the Fourteenth
Conference on Uncertainty in Artificial Intelligence (UAI-98)</em>,
pages 238-245, 1998.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="#">COCO: The Experimental Procedure</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
      Last updated on May 14, 2016.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.3.5.
    </div>
  </body>
</html>